{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nooralight/Conversational_Chatbot/blob/main/Chatbot_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGVASOEW4iN_"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "\n",
        "lines = open('movie_lines.txt', encoding='utf-8',\n",
        "             errors='ignore').read().split('\\n')\n",
        "\n",
        "convers = open('movie_conversations.txt', encoding='utf-8',\n",
        "             errors='ignore').read().split('\\n')\n",
        "\n",
        "\n",
        "exchn = []\n",
        "\n",
        "for conver in range(len(convers)):\n",
        "    exchn.append(convers[conver].split(' +++$+++ ')[-1][1:-1].replace(\"'\", \" \").replace(\",\",\"\").split())\n",
        "\n",
        "diag = {}\n",
        "for line in range(len(lines)):\n",
        "    diag[lines[line].split(' +++$+++ ')[0]] = lines[line].split(' +++$+++ ')[-1]\n",
        "\n",
        "\n",
        "\n",
        "questions = []\n",
        "answers = []\n",
        "\n",
        "for conver in exchn:\n",
        "    for i in range(len(conver) - 1):\n",
        "        questions.append(diag[conver[i]])\n",
        "        answers.append(diag[conver[i+1]])\n",
        "\n",
        "\n",
        "\n",
        "sorted_ques = []\n",
        "sorted_ans = []\n",
        "q_len= len(questions)\n",
        "for i in range(q_len):\n",
        "    if len(questions[i]) < 15:\n",
        "        sorted_ques.append(questions[i])\n",
        "        sorted_ans.append(answers[i])\n",
        "\n",
        "\n",
        "\n",
        "def clean_text(txt):\n",
        "    txt = txt.lower()\n",
        "    txt = re.sub(r\"i'm\", \"i am\", txt)\n",
        "    txt = re.sub(r\"he's\", \"he is\", txt)\n",
        "    txt = re.sub(r\"she's\", \"she is\", txt)\n",
        "    txt = re.sub(r\"that's\", \"that is\", txt)\n",
        "    txt = re.sub(r\"what's\", \"what is\", txt)\n",
        "    txt = re.sub(r\"where's\", \"where is\", txt)\n",
        "    txt = re.sub(r\"\\'ll\", \" will\", txt)\n",
        "    txt = re.sub(r\"\\'ve\", \" have\", txt)\n",
        "    txt = re.sub(r\"\\'re\", \" are\", txt)\n",
        "    txt = re.sub(r\"\\'d\", \" would\", txt)\n",
        "    txt = re.sub(r\"won't\", \"will not\", txt)\n",
        "    txt = re.sub(r\"can't\", \"can not\", txt)\n",
        "    txt = re.sub(r\"[^\\w\\s]\", \"\", txt)\n",
        "    return txt\n",
        "\n",
        "clean_ques = []\n",
        "clean_ans = []\n",
        "\n",
        "for line in range(len(sorted_ques)):\n",
        "    clean_ques.append(clean_text(sorted_ques[line]))\n",
        "        \n",
        "for line in range(len(sorted_ans)):\n",
        "    clean_ans.append(clean_text(sorted_ans[line]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for i in range(len(clean_ans)):\n",
        "    clean_ans[i] = ' '.join(clean_ans[i].split()[:15])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "clean_ans=clean_ans[:21000]\n",
        "clean_ques=clean_ques[:21000]\n",
        "\n",
        "\n",
        "word2count = {}\n",
        "\n",
        "for line in range(len(clean_ques)):\n",
        "    for word in clean_ques[line].split():\n",
        "        if word not in word2count:\n",
        "            word2count[word] = 1\n",
        "        else:\n",
        "            word2count[word] += 1\n",
        "for line in range(len(clean_ans)):\n",
        "    for word in clean_ans[line].split():\n",
        "        if word not in word2count:\n",
        "            word2count[word] = 1\n",
        "        else:\n",
        "            word2count[word] += 1\n",
        "\n",
        "\n",
        "\n",
        "vocab = {}\n",
        "word_num = 0\n",
        "for word, count in word2count.items():\n",
        "    if count >= 4:\n",
        "        vocab[word] = word_num\n",
        "        word_num += 1\n",
        "        \n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "for i in range(len(clean_ans)):\n",
        "    clean_ans[i] = '<SOS> ' + clean_ans[i] + ' <EOS>'\n",
        "\n",
        "\n",
        "\n",
        "tokens = ['<PAD>', '<EOS>', '<OUT>', '<SOS>']\n",
        "x = len(vocab)\n",
        "for token in range(len(tokens)):\n",
        "    vocab[tokens[token]] = x\n",
        "    x += 1\n",
        "    \n",
        "    \n",
        "\n",
        "vocab['cameron'] = vocab['<PAD>']\n",
        "vocab['<PAD>'] = 0\n",
        "\n",
        "\n",
        "inv_vocab = {w:v for v, w in vocab.items()}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "encoder_inp = []\n",
        "for line in range(len(clean_ques)):\n",
        "    lst = []\n",
        "    for word in clean_ques[line].split():\n",
        "        if word not in vocab:\n",
        "            lst.append(vocab['<OUT>'])\n",
        "        else:\n",
        "            lst.append(vocab[word])\n",
        "        \n",
        "    encoder_inp.append(lst)\n",
        "\n",
        "decoder_inp = []\n",
        "for line in range(len(clean_ans)):\n",
        "    lst = []\n",
        "    for word in clean_ans[line].split():\n",
        "        if word not in vocab:\n",
        "            lst.append(vocab['<OUT>'])\n",
        "        else:\n",
        "            lst.append(vocab[word])        \n",
        "    decoder_inp.append(lst)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "encoder_inp = pad_sequences(encoder_inp, 15, padding='post', truncating='post')\n",
        "decoder_inp = pad_sequences(decoder_inp, 15, padding='post', truncating='post')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "decoder_final_output = []\n",
        "for i in range(len(decoder_inp)):\n",
        "    decoder_final_output.append(decoder_inp[i][1:]) \n",
        "\n",
        "decoder_final_output = pad_sequences(decoder_final_output, 15, padding='post', truncating='post')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "decoder_final_output = to_categorical(decoder_final_output, len(vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIIH-z1oWD-F",
        "outputId": "58483d77-79c0-4afe-cb32-366fe158370d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "657/657 [==============================] - 225s 337ms/step - loss: 3.1429 - acc: 0.4992\n",
            "Epoch 2/5\n",
            "657/657 [==============================] - 221s 337ms/step - loss: 2.7883 - acc: 0.5298\n",
            "Epoch 3/5\n",
            "657/657 [==============================] - 220s 335ms/step - loss: 2.6323 - acc: 0.5480\n",
            "Epoch 4/5\n",
            "657/657 [==============================] - 221s 336ms/step - loss: 2.5396 - acc: 0.5547\n",
            "Epoch 5/5\n",
            "657/657 [==============================] - 222s 337ms/step - loss: 2.4755 - acc: 0.5581\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9f4c822110>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Model\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Input\n",
        "\n",
        "\n",
        "enc_inp = Input(shape=(15, ))\n",
        "dec_inp = Input(shape=(15, ))\n",
        "\n",
        "\n",
        "VOCAB_SIZE = len(vocab)\n",
        "embed = Embedding(VOCAB_SIZE+1, output_dim=50, \n",
        "                  input_length=15,\n",
        "                  trainable=True                  \n",
        "                  )\n",
        "\n",
        "\n",
        "enc_embed = embed(enc_inp)\n",
        "enc_lstm = LSTM(400, return_sequences=True, return_state=True)\n",
        "enc_op, h, c = enc_lstm(enc_embed)\n",
        "enc_states = [h, c]\n",
        "\n",
        "\n",
        "\n",
        "dec_embed = embed(dec_inp)\n",
        "dec_lstm = LSTM(400, return_sequences=True, return_state=True)\n",
        "dec_op, _, _ = dec_lstm(dec_embed, initial_state=enc_states)\n",
        "\n",
        "dense = Dense(VOCAB_SIZE, activation='softmax')\n",
        "\n",
        "dense_op = dense(dec_op)\n",
        "\n",
        "model = Model([enc_inp, dec_inp], dense_op)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',metrics=['acc'],optimizer='adam')\n",
        "\n",
        "model.fit([encoder_inp, decoder_inp],decoder_final_output,epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "nIoYUQknWKAy",
        "outputId": "75b9e831-64a7-46d1-8308-8353f96a1f5f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ca688338acd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0menc_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menc_inp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'enc_inp' is not defined"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Model \n",
        "      \n",
        "\n",
        "enc_model = Model([enc_inp], enc_states)\n",
        "\n",
        "\n",
        "\n",
        "# decoder Model\n",
        "decoder_state_input_h = Input(shape=(400,))\n",
        "decoder_state_input_c = Input(shape=(400,))\n",
        "\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "\n",
        "decoder_outputs, state_h, state_c = dec_lstm(dec_embed , \n",
        "                                    initial_state=decoder_states_inputs)\n",
        "\n",
        "\n",
        "decoder_states = [state_h, state_c]\n",
        "\n",
        "\n",
        "dec_model = Model([dec_inp]+ decoder_states_inputs,\n",
        "                                      [decoder_outputs]+ decoder_states)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "print(\"Conversation started\")\n",
        "\n",
        "import numpy as np\n",
        "prepro1 = \"\"\n",
        "while prepro1 != 'quit':\n",
        "    prepro1  = input(\"Ask me : \")\n",
        "   \n",
        "\n",
        "    prepro1 = clean_text(prepro1)\n",
        "   \n",
        "\n",
        "    prepro = [prepro1]\n",
        "   \n",
        "\n",
        "    txt = []\n",
        "    for x in prepro:\n",
        "        # x = \"hello\"\n",
        "        lst = []\n",
        "        for y in x.split():\n",
        "            ## y = \"hello\"\n",
        "            try:\n",
        "                lst.append(vocab[y])\n",
        "                ## vocab['hello'] = 454\n",
        "            except:\n",
        "                lst.append(vocab['<OUT>'])\n",
        "        txt.append(lst)\n",
        "\n",
        "    ## txt = [[454]]\n",
        "    txt = pad_sequences(txt, 15, padding='post')\n",
        "\n",
        "    ## txt = [[454,0,0,0,.........13]]\n",
        "\n",
        "    stat = enc_model.predict( txt)\n",
        "\n",
        "    empty_target_seq = np.zeros( ( 1 , 1) )\n",
        "     ##   empty_target_seq = [0]\n",
        "\n",
        "\n",
        "    empty_target_seq[0, 0] = vocab['<SOS>']\n",
        "    ##    empty_target_seq = [255]\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_translation = ''\n",
        "\n",
        "    while not stop_condition :\n",
        "\n",
        "        dec_outputs , h, c= dec_model.predict([ empty_target_seq] + stat )\n",
        "        decoder_concat_input = dense(dec_outputs)\n",
        "        ## decoder_concat_input = [0.1, 0.2, .4, .0, ...............]\n",
        "\n",
        "        sampled_word_index = np.argmax( decoder_concat_input[0, -1, :] )\n",
        "        ## sampled_word_index = [2]\n",
        "\n",
        "        sampled_word = inv_vocab[sampled_word_index] + ' '\n",
        "\n",
        "        ## inv_vocab[2] = 'hi'\n",
        "        ## sampled_word = 'hi '\n",
        "\n",
        "        if sampled_word != '<EOS> ':\n",
        "            decoded_translation += sampled_word  \n",
        "\n",
        "        if sampled_word == '<EOS> ' or len(decoded_translation.split()) > 15:\n",
        "            stop_condition = True \n",
        "\n",
        "        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
        "        empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
        "        ## <SOS> - > hi\n",
        "        ## hi --> <EOS>\n",
        "        stat = [h, c]  \n",
        "\n",
        "    print(\"Chatbot initializing: \", decoded_translation )\n",
        "    print(\"*****++++*****\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Chatbot_Final.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}